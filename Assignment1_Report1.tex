% \documentclass[singlecolumn]{article}
\documentclass[a4paper]{article}

\usepackage{color}
\usepackage[top=1in,bottom=1in,left=1.2in,right=1.2in]{geometry}
\usepackage[small]{titlesec}

% For clickable links and references
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
}

% For superscript citations
\usepackage[superscript]{cite}

% For code blocks
\usepackage{graphicx}
\usepackage[]{minted}
\newenvironment{code}{\captionsetup{type=listing}}

% For paragraph spacing instead of indentation
\usepackage{parskip}

% For float barriers
\usepackage{placeins}

% Custom heading sizes
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}


\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO:} #1}}

\title{Assignment 1 \\ \begin{small}\url{https://github.com/AdemOdza/CS6320_Assignment1}\end{small}}
\author{Full Name \\ Net ID \and Full Name \\ Net ID \and Full Name \\ Net ID}
% For two authors:
\author{Group11 \and  Abhirup Mukherjee \\ AXM240026 \and Adem Odza \\ AXO180008  \and Buvana Seshathri \\ BXS240020 \and Samiha Kuncham \\SXK240025}
%\author{Full Name \\ Net ID \and Full Name \\ Net ID}

\date{}

\begin{document}
\maketitle
\input{begin.tex}


\section{Implementation Details}

% \begin{figure}[h]
% \centering
% \small
% \resizebox{0.5\columnwidth}{!}{
% \includegraphics{./figs/code.png}
% }
% \caption{Example of a Code Piece.}
% \label{fig:eg}
% \end{figure}


% \begin{listing}[ht]
% \begin{minted}[frame=single,framesep=10pt]{python}
%   import numpy  
%   print("this is a piece of code")
% \end{minted}
% \caption{Example of a Code Piece.}
% \label{lst:eg}
% \end{listing}


\subsection{Preprocessing Decisions}
We are given two plain-text corpora of reviews where each line is a single review. In order to compute the counts for the unigram and bigram, we need to tokenize the words into two flat Python lists with each word of the corpora being a single list element. We chose a list as our data structure since it is easily iterable and sliceable.

The following preprocessing decisions were made, along with the rationale behind each choice:

\begin{enumerate}
    % ---------- LOWERCASE ----------
    \item \textbf{Lowercasing} - We converted all the text to lower case to ensure that all the words with the same spelling are treated the same way. This process helped us in reducing the vocabulary size and it also helped the model generalize better.

    % ---------- REMOVE PUNCTUATION ----------
    \item \textbf{Removed Punctuation} - All punctuation is removed so that the focus is on the actual words in the text. This decision was taken since punctuation often doesn't contribute significant meaning in bag-of-words or similar models and it can introduce unnecessary noise. We used the regular expression module \texttt{re} from the Python library to remove the punctuation in the text.

    % ---------- TOKENIZATION ----------
    \item \textbf{Tokenization} - Since the data given to us is already tokenized with one review per line and the tokens separated by spaces, we simply split each line into words. This made it straightforward and easy to process each word individually.
\end{enumerate}

The preprocessing steps resulted in two flat Python lists called \texttt{train\_tokens} and \texttt{val\_tokens}.

\subsection{Unknown word handling}
The input to this step is a list of tokens (words), and the output is a modified list in which rare words are replaced with the token \texttt{UNK}. This ensures that infrequent words are handled consistently, reducing sparsity in the dataset and improving the robustness of the model.

The steps were carried out are as follows:
\begin{enumerate}
\item \textbf{Threshold Selection} - We set the threshold for rare words at 2. Since the dataset is relatively small, a threshold of 2 ensures that we do not lose excessive vocabulary information while still handling infrequent words effectively.
\item \textbf{Frequency Calculation} - A frequency dictionary was created to count occurrences of each token. For instance, if the token \textit{hotel} appears five times, then \texttt{freq['hotel'] = 5}.
\item \textbf{Replacement Rule} - If a token appears fewer than 2 times, it is replaced with \texttt{UNK}. Otherwise, the original token is retained.
\end{enumerate}
\begin{listing}[ht]
\begin{minted}[frame=single,framesep=10pt]{python}
# unk handling
def replace_with_unk(tokens, threshold=2):
    freq = Counter(tokens)
    return ['UNK' if freq[token] < threshold else token for token in tokens]
\end{minted}
\caption{Function to replace unknown words}
\label{lst:unknown word handling}
\end{listing}
The result is a token list where all rare words are replaced by \texttt{UNK}, helping the model handle unseen or low-frequency words better. 

\subsection{Unigram Counts}
We chose to create a dictionary of the form \texttt{\{'$w_i$':freq($w_i$)\}} where $w_i$ is a word in our vocabulary (key) and $\text{freq}(w_i)$ is its frequency in the training corpus (value). Python dictionaries offer constant time key insertions and lookups ($O(1)$) and are simple to work with. To create this dictionary, we initialized an empty dictionary \texttt{unigram\_freq} and iterated through the training tokens. If a word is not in the dictionary, then we add it to the dictionary with a frequency of 0. Otherwise, we update its frequency.

\subsection{Bigram Counts}
Next, we compute the bigram frequencies into a \texttt{bigram\_freq} dictionary where each key is a bigram and the value is its frequency. To create this dictionary, we initialize the empty \texttt{bigram\_freq} dictionary and iterate. We use the \texttt{zip} function to combine our \texttt{train\_tokens\_with\_stop} list and a copy of this list starting from the second word to create a sequence of tuples, where each tuple is a bigram. We then iterate over each tuple and follow the same counting logic as before: if the tuple is not in \texttt{bigram\_freq}, we assign its value as 0. Otherwise, we increment its count. This gives us a dictionary of the form \texttt{\{('$w_{i-1}$','$w_i$'):bigram frequency\}}.

\subsection{Smoothing}
Smoothing techniques are used to handle tokens (for unigrams) and pairs (for bigrams) that were not seen during the train phase. When a word/pair occurs during the test phase that has not been encountered in the train phase, probability of that token becomes zero. Zero probability results in \textbf{infinite perplexity}. To avoid this, we use smoothing techniques.
 
In this assignment, we have explored \textbf{add-k smoothing} techniques with different \texttt{k} values. \texttt{k} values used are \texttt{[0.001, 0.01, 0.1, 0.5, 1.0]}. Add-k smoothing with \texttt{k=1.0} is called \textbf{Laplace smoothing}.
 
Formulae for calculating Add-k smoothing:
 
\textbf{For Unigram}: $$ P(w) = \frac{\text{count}(w) + k}{N + k * V} \\[1em] $$
 
\textbf{For Bigram}: $$ P(w, v) = \frac{\text{count}(w, v) + k}{\text{count}(w) + k * V} $$
 
where $\text{count(w)}$ is the count of token $\text{w}$, $\text{count(w, v)}$ is the count for pair $\text{(w,v)$, $\text{N}$ is the corpus size, and $\text{V}$ is the vocabulary size

\begin{listing}[ht]
\begin{minted}[frame=single,framesep=10pt]{python}
def add_k_smoothing_unigram(c_w, N, V, k):
  return (c_w + k) / (N + k * V)

def add_k_smoothing_bigram(c_w_v, c_w, V, k):
  return (c_w_v + k) / (c_w + k * V)
\end{minted}
\caption{Functions to perform smoothing}
\label{lst:smoothing}
\end{listing}

The above functions were used to calculate smoothing. These functions are called during perplexity calculation and they return the smoothed probabilities.

\subsection{Perplexity}
We use the following formula for calculating perplexity:
$$ l = \frac{1}{N} \sum_{i=1}^N \log P(w_i | w_{i-1}, \dots, w_{i-n+1})$$
$$ PP = 2^{-l} $$
where $N$ is the size of the training set, $P(w_i | w_{i-1}, \dots, w_{i-n+1})$ is the probability of the $i$th $N$-Gram

This is implemented using two functions: one for unigrams and one for bigrams. Both functions take the \texttt{corpus} (preprocessed corpus), \texttt{unigram\_counts} (the number of unigrams in the training set), $\text{N}$ (training corpus size), $\text{V}$ (vocabulary size), and $\text{k}$ (smoothing parameter). The bigram perplexity function takes one additional parameter, \texttt{bigram\_counts} (the number of unigrams in the training set). \\

\begin{listing}[!htbp]
\begin{minted}[frame=single,framesep=10pt]{python}
def calculate_unigram_perplexity(corpus, unigram_counts, N, V, k):
    # M = test corpus size
    M = len(corpus)
    total_log_prob = 0.0

    for token in corpus:
        # set default value of count to 0 
        # if token not found in train set tokens
        c_w = unigram_counts.get(token, 0)
        prob = add_k_smoothing_unigram(c_w, N, V, k)
        total_log_prob += math.log2(prob)

    l = total_log_prob / M
    perplexity = math.exp2(-l) # 2.0 ** -l

    return perplexity
\end{minted}
\caption{The unigram perplexity function}
\label{lst:unigram_perplexity}
\end{listing}

\begin{listing}[!htbp]
\begin{minted}[frame=single,framesep=10pt]{python}
def calculate_bigram_perplexity(corpus, unigram_counts, bigram_counts, V, k):
    # Create bigrams from the validation corpus
    bigram_corpus = zip(corpus, corpus[1:])
    M = len(corpus)
    total_log_prob = 0.0

    for bigram in bigram_corpus:
        previous_word = bigram[0]
        # set default value of count to 0 
        # if bigram not found in train set bigrams
        c_w_v = bigram_counts.get(bigram, 0) 
        c_w = unigram_counts.get(previous_word, 0)
        prob = add_k_smoothing_bigram(c_w_v, c_w, V, k)
        total_log_prob += math.log2(prob)

    l = total_log_prob / M
    perplexity = math.exp2(-l) # 2.0 ** -l

    return perplexity
\end{minted}
\caption{The bigram perplexity function}
\label{lst:bigram_perplexity}
\end{listing}

The smoothed probabilities are retrieved using the \texttt{add\_k\_smoothing\_unigram} function, which gets the value of the \texttt{k} parameter and calculates the smoothed probabilities. We get the smoothed probability of every n-gram in the corpus, find their logarithm, and find their sum. We then divide that sum by $M$ to get the $l$ value. We then raise $2$ to the power of $-l$ before returning.

Similarly, when calculating the bigram perplexity, we pair up the words in the corpus using the \texttt{zip} tool and iterate through each tuple of words. We fetch the counts of the bigram and the count of the previous word and pass these as arguments to the \texttt{add\_k\_smoothing\_bigram} function along with $V$ and $k$. We get the smoothed probability of every bigram in the corpus, find their logarithm, and find their sum. We then divide that sum by $M$ to get the $l$ value. We then raise $2$ to the power of $-l$ before returning.

\FloatBarrier

\section{Evaluation, Analysis, and Findings}
For hyperparameter \texttt{k} for \textbf{add-k smoothing}, upon trying different values, we inferred that while \texttt{k=0.001} resulted in lower perplexities in the training set, \texttt{k=0.01} produced significantly better results (lower perplexities) for both bigram and unigram during the testing phase. This helped us evaluate model performance and mitigate over-smoothing.\\
 
\centerline{\textbf{Table 1:} Training set Perplexity with different k values in add-k smoothing}
\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{} & \textbf{$k=0.001$} & \textbf{$k=0.01$} & \textbf{$k=0.1$} & \textbf{$k=0.5$} & \textbf{$k=1.0$} \\
\hline
\textbf{Unigram} & 391.324 & 391.324 & 391.340 & 391.680 & 392.592 \\
\textbf{Bigram}  & 36.4174 & 51.723  & 122.885 & 303.366 & 459.282 \\
\hline
\end{tabular}
\end{table}

\vspace{1em}
\centerline{\textbf{Table 2:} Validation set Perplexity with different k values in add-k smoothing}
\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{} & \textbf{$k=0.001$} & \textbf{$k=0.01$} & \textbf{$k=0.1$} & \textbf{$k=0.5$} & \textbf{$k=1.0$} \\
\hline
\textbf{Unigram} & 756.657 & 670.337 & 594.195 & 547.696 & 530.667 \\
\textbf{Bigram}  & 425.511 & 293.423 & 341.753 & 542.233 & 703.129 \\
\hline
\end{tabular}
\end{table}



\section{Programming Libraries Used}
The following modules and libraries were used:
\begin{itemize}
    \item \texttt{\textbf{re}} - This module provides regular expression matching operations similar to those found in Perl.
    \item \texttt{\textbf{Counter}} - We used the \texttt{Counter} class from the \texttt{collections} module, which is a dict subclass for counting hashable objects.
    \item \texttt{\textbf{math}} - This module provides access to common mathematical functions and constants, including those defined by the C standard.
    \item \texttt{\textbf{tabulate}} - We used the \texttt{tabulate} which is a PyPI library that allows pretty-printing of tabular data.
\end{itemize}


\section{Contributions of members}
We began by preprocessing the dataset, a step handled by \textbf{Samiha}. This involved converting all words to lowercase, removing punctuation, and tokenizing the reviews into words.
Unknown or rare words were replaced with \texttt{UNK} tokens using a threshold of 2, and the processed tokens were stored in a list data structure.\\

Next, \textbf{Abhirup} computed unigram and bigram frequencies from this preprocessed data, producing two dictionaries: one for unigram frequencies and one for bigram frequencies. He also integrated the smoothing functions that give the smoothed probabilities inside the perplexity functions once they were ready\\

\textbf{Buvana} applied the add-\(k\) smoothing and Laplace smoothing techniques for the training data and the validation data. Additionally, she computed, analyzed, and tabulated the perplexity numbers for different \texttt{k} values.\\

Finally, \textbf{Adem} implemented the perplexity calculation formula. Two functions were written, one for unigram perplexity and another for bigram perplexity. He also removed code smells and ensured the end-to-end pipeline worked correctly.\\

All members put in equal efforts in completing both the source code and report for this assignment and we are thankful for each other's contributions.\\

\section{Feedback}
We feel that this assignment gave us a chance to apply the theory that was presented during the lectures as a hands-on exercise. It cleared our misconceptions and deepened our understanding of elementary language models. As such, we feel that the difficulty level of this assignment was appropriate when compared to the material covered in the lectures. We enjoyed working on this assignment and are thankful for our instructor Dr Xinya Du's guidance.


% \todo{
% % \begin{itemize}
%     \subsection{Smoothing (15\%)}
%     \subsection{Unknown word handling (15\%)}
% % \end{titemize}
% }


% \section{Eval, Analysis and Findings}
% \todo{to add (30\%)}


% \section{Others}
% \todo{to add (10\%)}



\end{document}
